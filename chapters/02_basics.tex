\chapter{Background}

Before getting into the details of our driver, some concepts central to any NVMe driver need to be explained in detail first. These being the specification of NVMe, how device drivers are written and operate in Linux, and our programming language of choice, Rust.

\section{Non-Volatile Memory Express}
Non-Volatile Memory Express (NVMe) itself ``is an open collection of standards and information to fully expose [\ldots] non-volatile memory in all types of computer environments''\footnote{\url{https://nvmexpress.org/about/}}. Relevant to the thesis is the NVMe specification, an open logical device interface specificiation for accessing non-volatile storage media attached via the PCIe bus. NVMe was designed to capitilize on the low latency and parallelism of SSDs, providing improvements over older storage interfaces such as SATA or SAS in terms of speed and latency. This specification defined several key components:
\begin{itemize}
        \item \textbf{NVMe commands}: the basic units of work that the host system uses to communicate with the NVMe device. These commands may involve I/O operations or administrative tasks.
        \item \textbf{Submission Queues (SQ)}: The host system places commands here to be processed by the NVMe device. Each NVMe device can support multiple SQs, enabling parallel command processing.
        \item \textbf{Completion Queues (CQ)}: The NVMe device places completion entries noticing that commands have to processed. Each completion queue is associated with a submission queue, however the specification allows multiple submission queues to be associated with a single completion queue.
\end{itemize}

The specification supports 1 Administrative submission and completion queue pair and up to 65535 I/O submission and completion queues, allowing for high scalability and the ability to handle high volumes of I/O requests. Both the submission and completion queue operate as ring buffers, with a maximum of 65536 entries.

\subsection{Submitting and completing requests}

% Communicating with a NVMe device connected requires interacting with its Administrative and I/O controllers. This interaction is done through the use of so-called submission and completion queues.

(TODO: How submitting commands work, and how completions are done --> tikz graphic)

\section{NVMe devices in Linux}
% TODO: kernel space vs. user space. how user space drivers work. dma? vfio?

\subsection{Device drivers in Linux}
In the Linux operating system, device drivers play a pivotal role in the management and operation of hardware. They serve as the interface between the hardware and the software, allowing the operating system and applications that run on it to interact with hardware components without needing to understand the details of how the hardware operates. This abstraction is vital for the development of a stable, portable, and scalable operating system.

Linux drivers interact with the kernel and hardware through a set of predefined kernel functions. This interface abstracts away the complexities of the hardware and provides a consistent programming interface for driver developers. A driver typically registers itself with the kernel to handle specific hardware device IDs, and the kernel then calls into the driver when the application or system requires operations on this device. The operations a driver can perform include initialization, reading data from or writing data to the device, and handling interrupts triggered by the hardware.

Drivers developed for Linux are traditionally written in C, which offers the performance and low-level system access necessary for interacting with hardware. However, the growing ecosystem around the Rust programming language, known for its focus on safety and concurrency, has begun to make inroads into areas traditionally dominated by C, including device driver development.


\subsection{Kernel space}
One of the fundamental concepts in Linux and Unix-like operating systems is the division between user space and kernel space. This division is crucial for system security and stability.

Being a monolithic kernel, Linux's operating system runs entirely in kernel space, meaning it has complete access to the hardware and can execute any CPU instruction and access any memory address. Kernel space is highly protected because faults here can cause the entire system to crash. Device drivers primarily operate in kernel space, allowing them to directly control hardware and execute privileged operations necessary for their tasks. For user space applications, Linux offers multiple APIs to access block devices, these being the standard file I/O API $\texttt{read()}$/$\texttt{write()}$, $\texttt{libaio}$, and $\texttt{io\_uring}$, the latter two enabling asynchronous read and write operations to block devices.

(TODO: context switches when using these apis?)

\subsection{User space}
An alternative approach to kernel space device drivers is a driver that runs entirely in user space; SPDK\footnote{\url{https://spdk.io}} being the de-facto standard for NVMe devices. The driver is able to access the device after memory mapping device files from user space. This design offers more flexibility, simplicity, and stability over kernel drivers, with access to debugging tools and overall less restrictions the development of user space drivers is much easier and the ability to use any programming language is also guaranteed. User space drivers are also less likely to cause system crashes or kernel panics due to bugs; faults in user space can often be handled gracefully, improving overall system stability. By avoiding context switches, these drivers can in theory achieve better performance than its kernel space counterparts. SPDK for example, relying on a poll-based architecture rather than interrupt-based.

Given these advantages, we chose to develop a user space NVMe driver rather than kernel space.

\section{Rust}

Rust is a modern systems programming language with a focus on safety, speed and concurrency. It was designed to provide memory saftey and thread safety guarantees through a unique onwership model without any performance pitfalls. These safety checks are done at compile time, eliminating common bugs like null-pointer dereferences and data races. These features make Rust ideal for developing (user space) device drivers where safety, effiency are paramount. Furthermore, Rust is a compiled language without a garbage collector, leading to much better performance compared to interpreted languages and less overhead compared to languages that have a garbage collector.


TODO: what is rust
