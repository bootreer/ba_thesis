\chapter{Implementation}\label{chapter:implementation}
Our driver implementation takes architectural inspiration from the driver implemented in Redox, as well as notes from SPDK; additionally, many utility functions are adapted from those in \texttt{ixy.rs} \cite{ixy.rs}. All NVMe commands and directives are implemented based on Revision 1.4 of the NVMe Specification \cite{nvme-spec}.

\section{User Space Drivers}
Like SPDK, vroom runs entirely in user space and implements zero-copy I/O operations, as well as a poll-based architecture; SPDK\footnote{\url{https://spdk.io}} being the de-facto standard for NVMe devices in high throughput environments. To write a user space driver, we use all the concepts explained in \autoref{chapter:basics}. The driver can access the device after memory mapping device files from user space, which offers more flexibility, simplicity, and stability over kernel drivers; with access to debugging tools and overall fewer restrictions, the development of user space drivers is much easier, and the ability to use any programming language is also guaranteed. User space drivers are also less likely to cause system crashes or kernel panics due to bugs; faults in user space can often be handled gracefully, improving overall system stability. By avoiding context switches, these drivers can reduce overall latency and increase throughput.

Given these advantages, we chose to develop a user space NVMe driver rather than kernel space.

\section{Memory-Mapped I/O}\label{section:MMIO}

\autoref{lst:mmap} shows how to implement memory-mapping a PCIe resource in Rust. In this example, we open the \texttt{resource0} file with read and write access and pass the file descriptor and its length to \texttt{libc::map}. As \texttt{libc::mmap} directly calls \texttt{mmap(2)}, the function call is wrapped in an \texttt{unsafe} block. The BARs are mapped as shared memory, so changes to the mapped memory are also written back to the file and vice-versa. If the function returns a null pointer or the length of the file is 0, we return an error. Otherwise, the pointer and the file length are returned as a pair.

\begin{lstlisting}[float, language=Rust,label=lst:mmap,caption=Memory mapping a PCIe resource in Rust]
pub fn map_resource(pci_addr: &str) -> Result<(*mut u8, usize), Error> {
    let path = format!("sys/bus/pci/devices/{}/resource0", pci_addr);

    let file = fs::OpenOptions::new().read(true).write(true).open(&path)?;
    let len = fs::metadata(&path)?.len() as usize;

    let ptr = unsafe {
        libc::mmap(
            ptr::null_mut(),
            len,
            libc::PROT_READ | libc::PROT_WRITE,
            libc::MAP_SHARED,
            file.as_raw_fd(),
            0,
        ) as *mut u8
    };

    if ptr.is_null() || len == 0 {
        Err("pci mapping failed".into())
    } else {
        Ok((ptr, len))
    }
}
\end{lstlisting}

\section{Direct Memory Access}
We use DMA to enable the transfer of data between the host system and the NVMe device. We initialise DMA memory for all submission and completion queues, as well as buffers that the device can read from and write to.
As PCIe devices access memory via physical addresses independently of the CPU, we require the buffers we use for DMA to stay in main memory. We can use \texttt{mlock(2)} to guarantee a memory page is locked in main memory; however, the mapping is not static for \qty{4}{\kibi\byte} pages, the standard page size on Linux. Instead, we make use of \qty{2}{\mebi\byte} huge pages for this, where the physical addresses are pinned in Linux \cite{user_space_net}.
Enabling the usage of huge pages on the operating system is done with the shell script \texttt{setup-hugetlbfs.sh}, which creates a mount point for huge pages and writes a number of huge pages to \texttt{sysfs} files. Now we can allocate memory by creating the file in the newly mounted directory and then memory map the file with \texttt{mmap(2)} and lock it in memory with \texttt{mlock(2)} by using the appropriate bindings in the \texttt{libc} crate. We then derive the physical memory address of the page through \texttt{/proc/self/pagemap}.

\begin{figure}[H]
  \centering
    \includegraphics[width=\textwidth]{figures/pagemap}
    \caption{Fields of a pagemap entry when the page is present in main memory}
    \label{fig:pagemap}
\end{figure}

\begin{lstlisting}[float, language=Rust, label=lst:virt_phys,caption=Translating a virtual address to its physical address]
fn virt_to_phys(addr: usize) -> Result<usize, Error> {
    let pagesize = unsafe { libc::sysconf(libc::_SC_PAGESIZE) } as usize;

    let mut file = fs::OpenOptions::new()
        .read(true)
        .open("/proc/self/pagemap")?;

    file.seek(io::SeekFrom::Start(
        (addr / pagesize * mem::size_of::<usize>()) as u64,
    ))?;

    let mut buffer = [0_u8; mem::size_of::<usize>()];
    file.read_exact(&mut buffer)?;

    let phys = unsafe {
      mem::transmute::<[u8; mem::size_of::<usize>()], usize>(buffer)
    };
    Ok((phys & 0x007F_FFFF_FFFF_FFFF) * pagesize + addr % pagesize)
}
\end{lstlisting}

The pagemap contains one 64-bit value for each virtual page; our huge page is in main memory, so the pagemap entry is structured as depicted \autoref{fig:pagemap}. Finding the relative index of the page is done by taking the virtual address of it and dividing it by the page size, which we use to locate the corresponding pagemap entry. Constructing the physical address from the pagemap entry and the virtual address is done by taking the Page Frame Number (Bits 0-54), multiplying that by the page size to get the physical address of the page, to which we add (\texttt{addr \% pagesize}), the offset within the physical page. For \qty{2}{\mebi\byte} pages, this offset is 0. \autoref{lst:virt_phys} shows how this is done in Rust.

One of the reasons vroom needs to run as root is due to requiring \texttt{CAP\_SYS\_ADMIN} to read the Page Frame Numbers ever since the Rowhammer vulnerability exploit, i.e. from Linux 4.0 onwards.

For DMA, we define the struct \texttt{Dma<T>}, with which we can allocate a memory-mapped huge page, encapsulating its virtual address type \texttt{*T} and physical address. We also define the trait \texttt{DmaSlice}, which allows us to iterate over chunks of the DMA memory and use a subsection of the memory via slicing. This trait is used for zero-copy I/O operations.

\section{Architecture}
Our overall goal was to design a lightweight driver where external dependencies are kept to a minimum. As such, we only require the crates \texttt{libc} for bindings to C library functions and \texttt{byteorder} for reading different-sized integers and to avoid unnecessary unsafe code from working with raw bytes.

\begin{lstlisting}[language=Rust,label=lst:nvmedevice,caption=NvmeDevice struct definition]
pub struct NvmeDevice {
    pci_addr: String,
    addr: *mut u8, // BAR address
    len: usize, // BAR length
    dstrd: u16, // Doorbell stride
    admin_sq: NvmeSubQueue, // Queues
    admin_cq: NvmeCompQueue,
    io_sq: NvmeSubQueue,
    io_cq: NvmeCompQueue,
    buffer: Dma<u8>,           // 2MiB of buffer
    prp_list: Dma<[u64; 512]>, // PRP list
    pub namespaces: HashMap<u32, NvmeNamespace>,
    pub stats: NvmeStats,
    q_id: u16,
}
\end{lstlisting}

The entry point to accessing an NVMe device on our driver is the \texttt{NvmeDevice} struct (\autoref{lst:nvmedevice}). Like in Redox, we designed this struct to be able to handle all NVMe operations, i.e. Administrative and I/O capabilities; however, as we haven't implemented \texttt{async}, we handle the request submission and completion polling from beginning to end, meaning requests currently operate synchronously.

To enable multithreaded I/O processing, we have defined \texttt{NvmeQueuePair}, so, akin to SPDK, each thread can have its own queue pair to handle reading and writing without needing locking access to a single instance of \texttt{NvmeDevice}. With the queue pair, submissions and completions can be handled independently of one another. Each queue pair encapsulates a unique identifier, a completion and submission queue. With unique identifiers, we guarantee each queue pair only writes to its corresponding doorbell registers, thus enabling lock-free multithreaded I/O to the NVMe device.

\section{Driver Initialisation}
In this section, we will go over the initialisation process of the driver, looking at what happens within the function \texttt{init()} in \texttt{lib.rs} and the functions it calls; this function returns an instance of \texttt{NvmeDevice} if nothing goes wrong.

Before any configuration and initialisation are done, we check if the PCIe device has the class id \texttt{0x0108}: \texttt{0x01} for mass storage device, \texttt{0x08} the NVMe subclass. We then unbind the kernel driver from the NVMe device by writing the PCIe address of the device to the \texttt{unbind} file in \texttt{sysfs}. This is followed by enabling the bus master and disabling interrupts by setting the appropriate bits in the PCI command register, thus enabling DMA and disabling interrupts entirely, as our driver is poll-based. At this point, we also initialise all the relevant structs required for the driver, e.g. admin and I/O queues.

The BARs of the NVMe device are then mapped into main memory, as described in \autoref{section:MMIO}. We then follow the initialisation procedure described in the NVMe specification. First, we disable the controller by setting the \texttt{EN} (enable) bit to 0 in the \texttt{CC} (Controller Configuration) register. We wait for the \texttt{ready} bit in the \texttt{CSTS} register to be set to 0, after which we can configure the controller. Configuring the controller involves setting register values to those we require, such as the attributes of the admin queues and command entries. All relevant offsets for the registers are stored in the enum \texttt{NvmeRegs32} and \texttt{NvmeRegs64}. After all the configuration is set, the controller is re-enabled by setting the \texttt{EN} bit to 1. Now, the NVMe controller is ready to process admin submissions.

% accessing register be like
\begin{lstlisting}[language=Rust, label=lst:reg32,caption=Writing to a 32 bit register]
fn set_reg32(&self, reg: u32, value: u32) {
    assert!(reg as usize <= self.len - 4, "memory access out of bounds");
    unsafe {
      std::ptr::write_volatile(
        (self.addr as usize + reg as usize) as *mut u32, value
      );
    }
}
\end{lstlisting}

Although we use unsafe functions to access the BARs of the NVMe device, with an assertion guard, we can make sure that all accesses to the registers are not out of bounds (see \autoref{lst:reg32}).

After this, we request an I/O completion queue, followed by a request for an I/O submission queue so the \texttt{NvmeDevice} can do I/O operations. Finally, we identify the namespaces, storing these in a \texttt{HashMap} in the \texttt{NvmeDevice}.

\section{I/O Operations}\label{subsection:io}
As all commands are processed through submission and completion queues, we have defined the structs \texttt{NvmeSubQueue}, and \texttt{NvmeCompQueue}, which store a \texttt{NvmeCommand} array, and \texttt{NvmeCompletion} array each on a huge page. The submission entry being 64 bytes in size means that at most 1024 entries fit on a \qty{2}{\mebi\byte} huge page; for simplicity, we have not implemented queues spanning multiple pages. Thus, the maximum number of outstanding submissions we currently support is 1023 per queue.

\begin{table}[H]
    \centering
    \begin{tabular}{lrr}
        \textbf{Name} & \textbf{Zero-Copy} & \textbf{Struct} \\
        \toprule
        \texttt{read()} & Yes & \texttt{NvmeDevice} \\ \hline
        \texttt{write()} & Yes & \texttt{NvmeDevice} \\ \hline
        \texttt{read\_copied()} & No & \texttt{NvmeDevice} \\ \hline
        \texttt{write\_copied()} & No & \texttt{NvmeDevice} \\ \hline
        \texttt{batched\_read()} & No & \texttt{NvmeDevice} \\ \hline
        \texttt{batched\_write()} & No & \texttt{NvmeDevice} \\ \hline
        \midrule
        \texttt{submit\_io()} & Yes & \texttt{NvmeQueuePair} \\ \hline
        \bottomrule
    \end{tabular}
    \caption{I/O methods in vroom}
    \label{tab:vroom-io}
\end{table}

\autoref{tab:vroom-io} contains an overview of all the methods responsible for reading or writing. We pass references to \texttt{impl DmaSlice} to the zero-copy methods, which are iterated over in chunks of \qty{8196}{\kibi\byte}, as larger chunks require the use of PRP lists. We avoid building PRP lists for each request for now, as these require DMA-able memory. Additionally, \texttt{submit\_io()} returns the number of requests added to the submission queue, as it is the caller's responsibility to make sure completions are handled with \texttt{complete\_io()}.

As for the non-zero-copy methods, we pass \texttt{[u8]} slices, which are copied chunk-wise to the DMA buffer in \texttt{NvmeDevice} for writes or copied from for reads. For these methods, we can iterate over the data buffer in larger chunks, more specifically \qty{512}{\kibi\byte} chunks, as we have already constructed a PRP list for the DMA buffer in the initialisation of \texttt{NvmeDevice}. The batched read and write methods split the requests into smaller chunks and submit these in batches, taking advantage of the parallel nature of NVMe SSDs.

Aside from the data buffer, the user also passed the logical block address (LBA) for the I/O operation. The length of data written is derived from the size of the buffer.
