\chapter{Evaluation}\label{chapter:eval}
TODO: plots!!!

\section{Setup}
All benchmarks are run on a system with an Intel Xeon E5-2660 with 251GiB of RAM running Ubuntu 23.10 with a 1TB Samsung Evo 970 Plus NVMe SSD.

\begin{table}
    \centering
    \begin{tabular} { ||c|c|c|| }
        \cline{1-3}
        Sequential read & \multicolumn{2}{|c||}{3500 MB/s} \\ \cline{1-3}
        Sequential write & \multicolumn{2}{|c||}{3300 MB/s} \\ \cline{1-3}
        \multirow{2}{*}{Queue Depth 1, Thread 1} & Random read & 19000 IOPS \\ \cline{2-3}
                                                    & Random write & 60000 IOPS \\ \cline{1-3}
        \multirow{2}{*}{Queue Depth 32, Thread 4} & Random read & 600K IOPS \\ \cline{2-3}
                                                    & Random write & 550K IOPS \\ \cline{1-3}
    \end{tabular}
    \caption{Samsung Evo 970 Plus performance limits as per the datasheet}
    \label{tab:evoplus}
\end{table}

In the following sections we will compare our driver's performance with the datasheet numbers in \autoref{tab:evoplus}, as well as against other storage engines: \texttt{libaio}, \texttt{io\_uring}, SPDK and the Linux file I/O API \texttt{pread}/\texttt{pwrite} (\texttt{psync}). Each I/O engine is tested by running a read or write workload over 900 seconds, with I/O unit sizes of 4KiB. These are tested using \texttt{fio}\footnote{\url{https://github.com/axboe/fio}} with the configuration in \autoref{lst:fio_conf}.

\begin{lstlisting}[float, label=lst:fio_conf, caption=\texttt{fio} configuration]
  [global]
  io_engine={spdk,io_uring,libaio,psync}
  rw={randread,randwrite,read,write}
  blocksize=4k
  direct=1
  norandommap=1
  runtime=900

  numjobs={1,4}
  queue_depth={1,32}
  group_reporting=1
\end{lstlisting}

The NVMe controller is aware when a device is empty and thus can perform read requests without actually performing a read operation. On an empty drive, the reported ``read'' performance will be much higher than the manufacturer's claims, hence all read tests are done on a full drive, i.e. each logical block address (LBA) has been written to at least once.

For writes, the SSD is cleared beforehand, such that it is in a comparable state for each test. Full NVMe SSDs report worsened write performances, due to mechanisms such as wear-levelling and write amplification.

\section{Throughput}

\section{Latency}

\section{Comparison with other I/O engines}
\subsection{Queue depth 1}


\subsection{Queue depth 32, Multi-threaded}
