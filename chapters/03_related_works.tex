\chapter{Related Work}\label{chapter:related}
Writing device drivers is nothing new; as such, we will go over the user space NVMe drivers implemented by SPDK, Redox and Redleaf, the latter two also written in Rust like vroom. Additionally, we will look at what I/O APIs Linux offers for high throughput.

\section{SPDK}
Originally developed by Intel, the Storage Performance Development Kit (SPDK) provides a large set of tools libraries for high-performance storage applications, at the centre of which is its NVMe driver. The driver itself is poll-based, zero-copy, and runs in user space. By eliminating interrupts and system calls entirely, SPDK achieves the highest performance out of all modern storage APIs \cite{storage_api}. However, its added complexity raises the question of whether it's possible to write a user space, poll-based driver in Rust that achieves similar performance while also focusing on simplicity.

\section{Redox}
Redox is a Unix-like operating system written entirely in Rust to be a ``robust, reliable and safe general-purpose operating system'' \cite{redox}. Its development began in 2015 by Jeremy Soller and is still being actively worked on at the time of writing.

Redox is based on a microkernel architecture, so many operating system functionalities run in user space, especially drivers. Its design focuses on minimalism and modularity, emphasising implementing as much of the operating system in user space as possible, improving system stability and security. Currently, Redox's NVMe driver employs an interrupt-driven architecture and supports asynchronous I/O to the NVMe device, using \texttt{Future}'s for interrupt handling.

There have been no performance evaluations of Redox's NVMe driver at the time of writing.

\section{RedLeaf}
Like Redox, RedLeaf \cite{redleaf-page} is also a microkernel operating system written in Rust. Developed by the University of Utah's Mars Research Group in 2020, RedLeaf's NVMe user space driver shares a similar structure to Redox's driver. Their benchmark results show that the RedLeaf driver can achieve an I/O throughput within 1\% of and, in some cases, even exceeding what SPDK achieves \cite{redleaf}. It is important to note that Redleaf's driver was only tested in its sequential reading and writing performance.

\section{Linux}
Linux provides various APIs for handling I/O operations, the most prominent for asynchronous I/O being \texttt{libaio} and \texttt{io\_uring}; the latter is still being actively worked on and improved upon.

With the use of \texttt{libaio}, applications can access block devices asynchronously. This centres around the two system calls \texttt{io\_submit()} and \texttt{io\_getevents()}. As the names suggest, the former submits I/O requests to the kernel, while the latter retrieves I/O completions. With two system calls required per request, there comes significant overhead from context switches, with data being copied from kernel to user space and vice versa.

\texttt{io\_uring}, on the other hand, ``implements a shared memory-mapped, queue-driven request/response processing framework'' \cite{storage_api} by implementing a submission and completion ring which is mapped into user space and shared with the kernel. An application can add submissions to the submission ring without any system calls; however, by default, it notifies the kernel about new entries in the ring with \texttt{io\_uring\_enter()}, similar to updating the submission queue doorbell in the NVMe specification. As the completion queue is mapped into user space, it is also possible to poll for completions by polling for new entries in the completion ring, the alternative is to wait for new completions with \texttt{io\_uring\_enter()}. \texttt{io\_uring} can also spawn a kernel thread, which polls for new submissions; thus, it can handle I/O without system calls. In this mode, \texttt{io\_uring} can achieve performance within 10\% of SPDK, albeit at a higher CPU usage, requiring CPU cores for the polling threads to achieve higher throughput \cite{storage_api}.
